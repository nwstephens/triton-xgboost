{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8147db66",
   "metadata": {},
   "source": [
    "## Optimizing Performance\n",
    "Triton offers several tools to help tune your model deployment parameters and optimize your target metrics, whether that be throughput, latency, device utilization, or some other measure of performance. Some of these optimizations depend on expected server load and whether inference requests will be submitted in batches or one at a time from clients. As we shall see, Triton's performance analysis tools allow you to test performance based on a wide range of anticipated scenarios and modify deployment parameters accordingly.\n",
    "\n",
    "For this example, we will make use of Triton's `perf_analyzer` [tool](https://github.com/triton-inference-server/server/blob/main/docs/perf_analyzer.md#performance-analyzer), which allows us to quickly measure throughput and latency based on different batch sizes and request concurrency. We'll start with a basic comparison of the performance of our large model deployed on CPU vs GPU with batch size 1 and no concurrency.\n",
    "\n",
    "All of the specific performance numbers here were obtained on a DGX-1 with 8 V100s and Triton 21.11, but your numbers may vary depending on available hardware and whether or not you chose to enable categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f6dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance of our large model on CPU.\n",
    "# By default, perf_analyzer uses batch size 1 and concurrency 1.\n",
    "!perf_analyzer -m model-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now get the same performance numbers for GPU execution\n",
    "!perf_analyzer -m model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b953ec1",
   "metadata": {},
   "source": [
    "Already, we can see that GPU execution offers substantially improved throughput at lower latency for this complex model, but let's see what happens when we look at higher batch sizes or request load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure performance with batch size 6 and a concurrrency of 6 for\n",
    "# request submissions\n",
    "!perf_analyzer -m model-cpu -b 6 --concurrency-range 6:6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f9de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perf_analyzer -m model -b 6 --concurrency-range 6:6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a6705f",
   "metadata": {},
   "source": [
    "As we can see, deployed on CPU, the model was able to offer a somewhat increased throughput at higher load, but latency increased dramatically. Meanwhile, the same model deployed on the GPU significantly increased its throughput with only a slight increase in latency.\n",
    "\n",
    "In order to maintain a tight latency budget on a CPU-only server under high request load, we would have to turn to a significantly less sophisticated model. Let's imagine that we were trying to keep our p99 latency under 2 ms on the DGX machine referred to above. On CPU, we can just barely stay under that budget with a batch size of 6 and concurrency of 6 on CPU. Deploying the same model on GPU with the same parameters, we can keep our p99 latency under 0.7 ms and offer 3.5X the throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ba9337",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!perf_analyzer -m model-cpu -b 6 --concurrency-range 6:6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a5592",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perf_analyzer -m model -b 6 --concurrency-range 6:6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9beba",
   "metadata": {},
   "source": [
    "Let's see how far we can push our large model on GPU while staying within our 2 ms latency budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ab7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!perf_analyzer -m model -b 80 --concurrency-range 8:8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956b745",
   "metadata": {},
   "source": [
    "On the GPU, this larger model can achieve 20X the throughput of the smaller model on CPU, allowing us to handle a substantially higher load. But of course throughput performance is only part of the picture. If our latency budget forces us to use a smaller model on CPU, how much worse will we do at actually detecting fraud? Let's compute results for the entire test dataset using the large and small models and then compare their precision-recall curves to see how much we may be losing by resorting to the smaller model for CPU deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f026d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424da1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_COUNT = 8\n",
    "\n",
    "def create_batches(arr):\n",
    "    # Determine how many chunks are needed to keep size <= max_batch_size\n",
    "    chunks = (\n",
    "        arr.shape[0] // max_batch_size +\n",
    "        int(bool(arr.shape[0] % max_batch_size) or arr.shape[0] < max_batch_size)\n",
    "    )\n",
    "    return np.array_split(arr, max(GPU_COUNT, chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431cdd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model_results = np.concatenate([triton_predict('large_model', chunk) for chunk in create_batches(np_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c19d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model_results = np.concatenate([triton_predict('small_model-cpu', chunk) for chunk in create_batches(np_data)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3862ec0",
   "metadata": {},
   "source": [
    "Note that we can more quickly process the full dataset on GPU even with a significantly more sophisticated model  than we are using for our CPU deployment. As an interesting point of comparison, due to the optimized inference performance of the RAPIDS Forest Inference Library (FIL) used by the Triton backend and Triton's inherent ability to parallelize over available GPUs, it is even faster to submit these samples for processing to Triton than it is to process them locally using XGBoost for the larger model, despite the overhead of data transfer. For information about invoking FIL directly in Python without Triton, see the [FIL documentation](https://github.com/rapidsai/cuml/tree/branch-21.12/python/cuml/fil#fil---rapids-forest-inference-library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0094559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time large_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a8bb1",
   "metadata": {},
   "source": [
    "We now return to evaluating the benefit of the larger model for accurately detecting fraud by computing precision-recall curves for both the small and large models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c5a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_precision, large_recall, _ = cuml.metrics.precision_recall_curve(y_test, large_model_results[:, 1])\n",
    "small_precision, small_recall, _ = cuml.metrics.precision_recall_curve(y_test, small_model_results[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a855acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fe250",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(small_precision, small_recall, color='#0071c5')\n",
    "plt.plot(large_precision, large_recall, color='#76b900')\n",
    "plt.title('Precision vs Recall for Small and Large Models')\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36712d5b",
   "metadata": {},
   "source": [
    "As we can see, the larger, more sophisticated model dominates the smaller model all along this curve. By deploying our model on GPU, we can identify a far greater proportion of actual fraud incidents with fewer false positives, all without going over our latency budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the server\n",
    "!docker rm -f tritonserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747b1fb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this example notebook, we showed how to deploy an XGBoost model in Triton using the new FIL backend. While it is possible to deploy these models on both CPU and GPU in Triton, GPU-deployed models offer far higher throughput at lower latency. As a result, we can deploy more sophisticated models on the GPU for any given latency budget and thereby obtain far more accurate results.\n",
    "\n",
    "While we have focused on XGBoost in this example, FIL also natively supports LightGBM's text serialization format as well as Treelite's checkpoint format. Thus, the same general steps can be used to serve LightGBM models and any Treelite-convertible model (including Scikit-Learn and cuML forest models). With the new FIL backend, Triton is now ready to serve forest models of all kinds in production, whether on their own or in concert with any of the deep-learning models supported by Triton."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
