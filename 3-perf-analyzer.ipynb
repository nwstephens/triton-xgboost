{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac5f556",
   "metadata": {},
   "source": [
    "## Optimizing Performance\n",
    "Triton offers several tools to help tune your model deployment parameters and optimize your target metrics, whether that be throughput, latency, device utilization, or some other measure of performance. Some of these optimizations depend on expected server load and whether inference requests will be submitted in batches or one at a time from clients. Triton's performance analysis tools allow you to test performance based on a wide range of anticipated scenarios and modify deployment parameters accordingly. For this example, we will make use of Triton's `perf_analyzer` [tool](https://github.com/triton-inference-server/server/blob/main/docs/perf_analyzer.md#performance-analyzer), which allows us to quickly measure throughput and latency based on different batch sizes and request concurrency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7922dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tritonclient[all]\n",
    "!apt update\n",
    "!apt install libb64-0d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2b74540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 6\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Latency limit: 0 msec\n",
      "  Concurrency limit: 6 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 6\n",
      "  Client: \n",
      "    Request count: 14331\n",
      "    Throughput: 17197.2 infer/sec\n",
      "    Avg latency: 2091 usec (standard deviation 639 usec)\n",
      "    p50 latency: 1954 usec\n",
      "    p90 latency: 2396 usec\n",
      "    p95 latency: 2820 usec\n",
      "    p99 latency: 4613 usec\n",
      "    Avg HTTP time: 2093 usec (send/recv 41 usec + response wait 2052 usec)\n",
      "  Server: \n",
      "    Inference count: 103002\n",
      "    Execution count: 5843\n",
      "    Successful request count: 17167\n",
      "    Avg request latency: 1872 usec (overhead 864 usec + queue 837 usec + compute input 1 usec + compute infer 165 usec + compute output 5 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 6, throughput: 17197.2 infer/sec, latency 2091 usec\n"
     ]
    }
   ],
   "source": [
    "# CPU serving with 6 batches and 6 concurrent requests results in high throughput but with high latency\n",
    "!perf_analyzer -m model-cpu -b 6 --concurrency-range 6:6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c02ddd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 6\n",
      "  Using \"time_windows\" mode for stabilization\n",
      "  Measurement window: 5000 msec\n",
      "  Latency limit: 0 msec\n",
      "  Concurrency limit: 6 concurrent requests\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 6\n",
      "  Client: \n",
      "    Request count: 62835\n",
      "    Throughput: 75402 infer/sec\n",
      "    Avg latency: 476 usec (standard deviation 189 usec)\n",
      "    p50 latency: 464 usec\n",
      "    p90 latency: 520 usec\n",
      "    p95 latency: 559 usec\n",
      "    p99 latency: 783 usec\n",
      "    Avg HTTP time: 474 usec (send/recv 34 usec + response wait 440 usec)\n",
      "  Server: \n",
      "    Inference count: 452454\n",
      "    Execution count: 17269\n",
      "    Successful request count: 75409\n",
      "    Avg request latency: 245 usec (overhead 104 usec + queue 123 usec + compute input 5 usec + compute infer 6 usec + compute output 7 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 6, throughput: 75402 infer/sec, latency 476 usec\n"
     ]
    }
   ],
   "source": [
    "# By comparison, GPU serving results in higher throughput with lower latency\n",
    "!perf_analyzer -m model -b 6 --concurrency-range 6:6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e3be4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In these notebooks, we showed how to deploy an XGBoost model in Triton using the new FIL backend. While it is possible to deploy these models on both CPU and GPU in Triton, GPU-deployed models offer far higher throughput at lower latency. As a result, we can deploy more sophisticated models on the GPU for any given latency budget and thereby obtain far more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241c2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
