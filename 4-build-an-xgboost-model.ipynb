{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1109ca",
   "metadata": {},
   "source": [
    "# Fraud Detection with XGBoost\n",
    "\n",
    "*This notebook is adapted from [Real-time Serving for XGBoost, Scikit-Learn RandomForest, LightGBM, and More](https://developer.nvidia.com/blog/real-time-serving-for-xgboost-scikit-learn-randomforest-lightgbm-and-more/).*\n",
    "\n",
    "In this example notebook, we will go step-by-step through the process of training and deploying an XGBoost fraud detection model using Triton's new FIL backend. Along the way, we'll show how to analyze the performance of a model deployed in Triton and optimize its performance based on specific SLA targets or other considerations. You can use Jupyter Lab running on the PyTorch container. If you haven't already installed the the PyTorch see [Set up PyTorch and Triton Containers]()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c9ecf2",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "For this example, we will make use of data from the [IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection/overview) Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da31a497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_transaction.csv\n"
     ]
    }
   ],
   "source": [
    "!tar xzvf data/train_transaction.tgz -C data/\n",
    "train_csv = 'data/train_transaction.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee26db3e",
   "metadata": {},
   "source": [
    "## Training Example Models\n",
    "While the IEEE-CIS Kaggle competition focused on a more sophisticated problem involving analysis of both fraudulent transactions and the users linked to those transactions, we will use a simpler version of that problem (identifying fraudulent transactions only) to build our example model. In the following steps, we make use of cuML's preprocessing tools to clean the data and then train two example models using XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab9cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.preprocessing import SimpleImputer\n",
    "from cuml.preprocessing import LabelEncoder\n",
    "# Due to an upstream bug, cuML's train_test_split function is\n",
    "# currently non-deterministic. We will therefore use sklearn's\n",
    "# train_test_split in this example to obtain more consistent\n",
    "# results.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b0246b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV files into cuDF DataFrames\n",
    "data = cudf.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2929def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs in data\n",
    "nan_columns = data.columns[data.isna().any().to_pandas()]\n",
    "float_nan_subset = data[nan_columns].select_dtypes(include='float64')\n",
    "\n",
    "imputer = SimpleImputer(missing_values=cp.nan, strategy='median')\n",
    "data[float_nan_subset.columns] = imputer.fit_transform(float_nan_subset)\n",
    "\n",
    "obj_nan_subset = data[nan_columns].select_dtypes(include='object')\n",
    "data[obj_nan_subset.columns] = obj_nan_subset.fillna('UNKNOWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8daedba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string columns to categorical or perform label encoding\n",
    "cat_columns = data.select_dtypes(include='object')\n",
    "for col in cat_columns.columns:\n",
    "    data[col] = LabelEncoder().fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d778451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = data.drop('isFraud', axis=1)\n",
    "y = data.isFraud.astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.to_pandas(), y.to_pandas(), test_size=0.3, stratify=y.to_pandas(), random_state=SEED\n",
    ")\n",
    "# Copy data to avoid slowdowns due to fragmentation\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1232a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "669f5823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model training function\n",
    "def train_model(num_trees, max_depth):\n",
    "    model = xgb.XGBClassifier(\n",
    "        tree_method='gpu_hist',\n",
    "        use_label_encoder=False,\n",
    "        predictor='gpu_predictor',\n",
    "        eval_metric='aucpr',\n",
    "        objective='binary:logistic',\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=num_trees\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_test, y_test)]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8046fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-aucpr:0.27398\n",
      "[1]\tvalidation_0-aucpr:0.31137\n",
      "[2]\tvalidation_0-aucpr:0.36501\n",
      "[3]\tvalidation_0-aucpr:0.37411\n",
      "[4]\tvalidation_0-aucpr:0.38408\n",
      "[5]\tvalidation_0-aucpr:0.41039\n",
      "[6]\tvalidation_0-aucpr:0.41761\n",
      "[7]\tvalidation_0-aucpr:0.42502\n",
      "[8]\tvalidation_0-aucpr:0.43361\n",
      "[9]\tvalidation_0-aucpr:0.44091\n",
      "[10]\tvalidation_0-aucpr:0.44511\n",
      "[11]\tvalidation_0-aucpr:0.44798\n",
      "[12]\tvalidation_0-aucpr:0.45469\n",
      "[13]\tvalidation_0-aucpr:0.45892\n",
      "[14]\tvalidation_0-aucpr:0.46160\n",
      "[15]\tvalidation_0-aucpr:0.46531\n",
      "[16]\tvalidation_0-aucpr:0.46899\n",
      "[17]\tvalidation_0-aucpr:0.47269\n",
      "[18]\tvalidation_0-aucpr:0.47758\n",
      "[19]\tvalidation_0-aucpr:0.48164\n",
      "[20]\tvalidation_0-aucpr:0.48452\n",
      "[21]\tvalidation_0-aucpr:0.48778\n",
      "[22]\tvalidation_0-aucpr:0.49041\n",
      "[23]\tvalidation_0-aucpr:0.49339\n",
      "[24]\tvalidation_0-aucpr:0.49637\n",
      "[25]\tvalidation_0-aucpr:0.49816\n",
      "[26]\tvalidation_0-aucpr:0.50005\n",
      "[27]\tvalidation_0-aucpr:0.50383\n",
      "[28]\tvalidation_0-aucpr:0.50801\n",
      "[29]\tvalidation_0-aucpr:0.51021\n",
      "[30]\tvalidation_0-aucpr:0.51175\n",
      "[31]\tvalidation_0-aucpr:0.51372\n",
      "[32]\tvalidation_0-aucpr:0.51562\n",
      "[33]\tvalidation_0-aucpr:0.51770\n",
      "[34]\tvalidation_0-aucpr:0.52007\n",
      "[35]\tvalidation_0-aucpr:0.52163\n",
      "[36]\tvalidation_0-aucpr:0.52451\n",
      "[37]\tvalidation_0-aucpr:0.52614\n",
      "[38]\tvalidation_0-aucpr:0.52841\n",
      "[39]\tvalidation_0-aucpr:0.52999\n",
      "[40]\tvalidation_0-aucpr:0.53058\n",
      "[41]\tvalidation_0-aucpr:0.53206\n",
      "[42]\tvalidation_0-aucpr:0.53293\n",
      "[43]\tvalidation_0-aucpr:0.53374\n",
      "[44]\tvalidation_0-aucpr:0.53558\n",
      "[45]\tvalidation_0-aucpr:0.53665\n",
      "[46]\tvalidation_0-aucpr:0.53923\n",
      "[47]\tvalidation_0-aucpr:0.54017\n",
      "[48]\tvalidation_0-aucpr:0.54133\n",
      "[49]\tvalidation_0-aucpr:0.54312\n",
      "[50]\tvalidation_0-aucpr:0.54413\n",
      "[51]\tvalidation_0-aucpr:0.54540\n",
      "[52]\tvalidation_0-aucpr:0.54615\n",
      "[53]\tvalidation_0-aucpr:0.54682\n",
      "[54]\tvalidation_0-aucpr:0.54747\n",
      "[55]\tvalidation_0-aucpr:0.54882\n",
      "[56]\tvalidation_0-aucpr:0.55010\n",
      "[57]\tvalidation_0-aucpr:0.55057\n",
      "[58]\tvalidation_0-aucpr:0.55189\n",
      "[59]\tvalidation_0-aucpr:0.55297\n",
      "[60]\tvalidation_0-aucpr:0.55487\n",
      "[61]\tvalidation_0-aucpr:0.55553\n",
      "[62]\tvalidation_0-aucpr:0.55636\n",
      "[63]\tvalidation_0-aucpr:0.55735\n",
      "[64]\tvalidation_0-aucpr:0.55821\n",
      "[65]\tvalidation_0-aucpr:0.55852\n",
      "[66]\tvalidation_0-aucpr:0.55955\n",
      "[67]\tvalidation_0-aucpr:0.56031\n",
      "[68]\tvalidation_0-aucpr:0.56071\n",
      "[69]\tvalidation_0-aucpr:0.56124\n",
      "[70]\tvalidation_0-aucpr:0.56271\n",
      "[71]\tvalidation_0-aucpr:0.56300\n",
      "[72]\tvalidation_0-aucpr:0.56389\n",
      "[73]\tvalidation_0-aucpr:0.56450\n",
      "[74]\tvalidation_0-aucpr:0.56512\n",
      "[75]\tvalidation_0-aucpr:0.56610\n",
      "[76]\tvalidation_0-aucpr:0.56825\n",
      "[77]\tvalidation_0-aucpr:0.56988\n",
      "[78]\tvalidation_0-aucpr:0.57006\n",
      "[79]\tvalidation_0-aucpr:0.57030\n",
      "[80]\tvalidation_0-aucpr:0.57073\n",
      "[81]\tvalidation_0-aucpr:0.57117\n",
      "[82]\tvalidation_0-aucpr:0.57169\n",
      "[83]\tvalidation_0-aucpr:0.57214\n",
      "[84]\tvalidation_0-aucpr:0.57287\n",
      "[85]\tvalidation_0-aucpr:0.57312\n",
      "[86]\tvalidation_0-aucpr:0.57449\n",
      "[87]\tvalidation_0-aucpr:0.57520\n",
      "[88]\tvalidation_0-aucpr:0.57528\n",
      "[89]\tvalidation_0-aucpr:0.57585\n",
      "[90]\tvalidation_0-aucpr:0.57682\n",
      "[91]\tvalidation_0-aucpr:0.57713\n",
      "[92]\tvalidation_0-aucpr:0.57807\n",
      "[93]\tvalidation_0-aucpr:0.57863\n",
      "[94]\tvalidation_0-aucpr:0.57944\n",
      "[95]\tvalidation_0-aucpr:0.58112\n",
      "[96]\tvalidation_0-aucpr:0.58228\n",
      "[97]\tvalidation_0-aucpr:0.58256\n",
      "[98]\tvalidation_0-aucpr:0.58267\n",
      "[99]\tvalidation_0-aucpr:0.58378\n",
      "[100]\tvalidation_0-aucpr:0.58468\n",
      "[101]\tvalidation_0-aucpr:0.58552\n",
      "[102]\tvalidation_0-aucpr:0.58615\n",
      "[103]\tvalidation_0-aucpr:0.58648\n",
      "[104]\tvalidation_0-aucpr:0.58690\n",
      "[105]\tvalidation_0-aucpr:0.58729\n",
      "[106]\tvalidation_0-aucpr:0.58756\n",
      "[107]\tvalidation_0-aucpr:0.58808\n",
      "[108]\tvalidation_0-aucpr:0.58845\n",
      "[109]\tvalidation_0-aucpr:0.58859\n",
      "[110]\tvalidation_0-aucpr:0.58876\n",
      "[111]\tvalidation_0-aucpr:0.58915\n",
      "[112]\tvalidation_0-aucpr:0.59020\n",
      "[113]\tvalidation_0-aucpr:0.59085\n",
      "[114]\tvalidation_0-aucpr:0.59165\n",
      "[115]\tvalidation_0-aucpr:0.59281\n",
      "[116]\tvalidation_0-aucpr:0.59365\n",
      "[117]\tvalidation_0-aucpr:0.59462\n",
      "[118]\tvalidation_0-aucpr:0.59466\n",
      "[119]\tvalidation_0-aucpr:0.59513\n",
      "[120]\tvalidation_0-aucpr:0.59543\n",
      "[121]\tvalidation_0-aucpr:0.59565\n",
      "[122]\tvalidation_0-aucpr:0.59627\n",
      "[123]\tvalidation_0-aucpr:0.59671\n",
      "[124]\tvalidation_0-aucpr:0.59725\n",
      "[125]\tvalidation_0-aucpr:0.59780\n",
      "[126]\tvalidation_0-aucpr:0.59855\n",
      "[127]\tvalidation_0-aucpr:0.59901\n",
      "[128]\tvalidation_0-aucpr:0.59964\n",
      "[129]\tvalidation_0-aucpr:0.59998\n",
      "[130]\tvalidation_0-aucpr:0.60040\n",
      "[131]\tvalidation_0-aucpr:0.60057\n",
      "[132]\tvalidation_0-aucpr:0.60095\n",
      "[133]\tvalidation_0-aucpr:0.60135\n",
      "[134]\tvalidation_0-aucpr:0.60154\n",
      "[135]\tvalidation_0-aucpr:0.60211\n",
      "[136]\tvalidation_0-aucpr:0.60237\n",
      "[137]\tvalidation_0-aucpr:0.60270\n",
      "[138]\tvalidation_0-aucpr:0.60359\n",
      "[139]\tvalidation_0-aucpr:0.60423\n",
      "[140]\tvalidation_0-aucpr:0.60441\n",
      "[141]\tvalidation_0-aucpr:0.60462\n",
      "[142]\tvalidation_0-aucpr:0.60498\n",
      "[143]\tvalidation_0-aucpr:0.60552\n",
      "[144]\tvalidation_0-aucpr:0.60603\n",
      "[145]\tvalidation_0-aucpr:0.60662\n",
      "[146]\tvalidation_0-aucpr:0.60693\n",
      "[147]\tvalidation_0-aucpr:0.60726\n",
      "[148]\tvalidation_0-aucpr:0.60764\n",
      "[149]\tvalidation_0-aucpr:0.60850\n",
      "[150]\tvalidation_0-aucpr:0.60889\n",
      "[151]\tvalidation_0-aucpr:0.60919\n",
      "[152]\tvalidation_0-aucpr:0.60941\n",
      "[153]\tvalidation_0-aucpr:0.60988\n",
      "[154]\tvalidation_0-aucpr:0.61012\n",
      "[155]\tvalidation_0-aucpr:0.61040\n"
     ]
    }
   ],
   "source": [
    "# Train a small model with just 500 trees and a maximum depth of 3\n",
    "model = train_model(300, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5c3490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up some room on the GPU by explicitly deleting dataframes\n",
    "import gc\n",
    "del data\n",
    "del nan_columns\n",
    "del float_nan_subset\n",
    "del imputer\n",
    "del obj_nan_subset\n",
    "del cat_columns\n",
    "del X\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26acb194",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_pickle('X_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4658e914",
   "metadata": {},
   "source": [
    "### Model Serialization\n",
    "Triton models can be stored locally on disk or in S3, Google Cloud Storage, or Azure Storage. For this example, we will stick to local storage, but information about using cloud storage solutions can be found [here](https://github.com/triton-inference-server/server/blob/main/docs/model_repository.md). Each model has a dedicated directory within a main model repository directory. Multiple versions of a model can also be served by Triton, as indicated by numbered directories (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8a1bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create the model repository directory. The name of this directory is arbitrary.\n",
    "MODEL_REPO_PATH = '/workspace/volume1/model_repository'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bccb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_model(model, model_name):\n",
    "    # The name of the model directory determines the name of the model as reported\n",
    "    # by Triton\n",
    "    model_dir = os.path.join(MODEL_REPO_PATH, model_name)\n",
    "    # We can store multiple versions of the model in the same directory. In our\n",
    "    # case, we have just one version, so we will add a single directory, named '1'.\n",
    "    version_dir = os.path.join(model_dir, '1')\n",
    "    os.makedirs(version_dir, exist_ok=True)\n",
    "    \n",
    "    # The default filename for XGBoost models saved in json format is 'xgboost.json'.\n",
    "    # It is recommended that you use this filename to avoid having to specify a\n",
    "    # name in the configuration file.\n",
    "    model_file = os.path.join(version_dir, 'xgboost.json')\n",
    "    model.save_model(model_file)\n",
    "    \n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = serialize_model(model, 'model')\n",
    "model_cpu_dir = serialize_model(model, 'model-cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c51bb8",
   "metadata": {},
   "source": [
    "### The Configuration File\n",
    "The configuration file associated with a model tells Triton a little bit about the model itself and how you would like to use it. You can read about all generic Triton configuration options [here](https://github.com/triton-inference-server/server/blob/master/docs/model_configuration.md) and about configuration options specific to the FIL backend [here](https://github.com/triton-inference-server/fil_backend#configuration), but we will focus on just a few of the most common and relevant options in this example. Below are general descriptions of these options:\n",
    "- **max_batch_size**: The maximum batch size that can be passed to this model. In general, the only limit on the size of batches passed to a FIL backend is the memory available with which to process them. For GPU execution, the available memory is determined by the size of Triton's CUDA memory pool, which can be set via a command line argument when starting the server.\n",
    "- **input**: Options in this section tell Triton the number of features to expect for each input sample.\n",
    "- **output**: Options in this section tell Triton how many output values there will be for each sample. If the \"predict_proba\" option (described further on) is set to true, then a probability value will be returned for each class. Otherwise, a single value will be returned indicating the class predicted for the given sample.\n",
    "- **instance_group**: This determines how many instances of this model will be created and whether they will use the GPU or CPU.\n",
    "- **model_type**: A string indicating what format the model is in (\"xgboost_json\" in this example, but \"xgboost\", \"lightgbm\", and \"tl_checkpoint\" are valid formats as well).\n",
    "- **predict_proba**: If set to true, probability values will be returned for each class rather than just a class prediction.\n",
    "- **output_class**: True for classification models, false for regression models.\n",
    "- **threshold**: A score threshold for determining classification. When output_class is set to true, this must be provided, although it will not be used if predict_proba is also set to true.\n",
    "- **storage_type**: In general, using \"AUTO\" for this setting should meet most usecases. If \"AUTO\" storage is selected, FIL will load the model using either a sparse or dense representation based on the approximate size of the model. In some cases, you may want to explicitly set this to \"SPARSE\" in order to reduce the memory footprint of large models.\n",
    "\n",
    "Based on this information, let's set up configuration files for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8971fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum size in bytes for input and output arrays. If you are\n",
    "# using Triton 21.11 or higher, all memory allocations will make\n",
    "# use of Triton's memory pool, which has a default size of\n",
    "# 67_108_864 bytes. This can be increased using the\n",
    "# `--cuda-memory-pool-byte-size` option when the server is\n",
    "# started, but this notebook should work fine with default\n",
    "# settings.\n",
    "MAX_MEMORY_BYTES = 60_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac81495",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_test.shape[1]\n",
    "num_classes = cp.unique(y_test).size\n",
    "bytes_per_sample = (features + num_classes) * 4\n",
    "max_batch_size = MAX_MEMORY_BYTES // bytes_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4099b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_config(model_dir, deployment_type='gpu', storage_type='AUTO'):\n",
    "    if deployment_type.lower() == 'cpu':\n",
    "        instance_kind = 'KIND_CPU'\n",
    "    else:\n",
    "        instance_kind = 'KIND_GPU'\n",
    "\n",
    "    config_text = f\"\"\"backend: \"fil\"\n",
    "max_batch_size: {max_batch_size}\n",
    "input [                                 \n",
    " {{  \n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {features} ]                    \n",
    "  }} \n",
    "]\n",
    "output [\n",
    " {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {num_classes} ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: {instance_kind} }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"xgboost_json\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"predict_proba\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"threshold\"\n",
    "    value: {{ string_value: \"0.5\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"storage_type\"\n",
    "    value: {{ string_value: \"{storage_type}\" }}\n",
    "  }}\n",
    "]\n",
    "\n",
    "dynamic_batching {{\n",
    "  max_queue_delay_microseconds: 100\n",
    "}}\"\"\"\n",
    "    config_path = os.path.join(model_dir, 'config.pbtxt')\n",
    "    with open(config_path, 'w') as file_:\n",
    "        file_.write(config_text)\n",
    "\n",
    "    return config_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3147a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_config(model_dir, deployment_type='gpu')\n",
    "generate_config(model_cpu_dir, deployment_type='cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
