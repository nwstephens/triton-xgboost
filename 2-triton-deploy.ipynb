{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06a356b1",
   "metadata": {},
   "source": [
    "# Deploying with Triton Inference Server\n",
    "\n",
    "*This notebook is adapted from [Real-time Serving for XGBoost, Scikit-Learn RandomForest, LightGBM, and More](https://developer.nvidia.com/blog/real-time-serving-for-xgboost-scikit-learn-randomforest-lightgbm-and-more/).*\n",
    "\n",
    "Now that we have a model to work with, let's deploy it for real-time serving using Triton. In order to do so, we will need to first serialize the models in the directory structure that Triton expects and then add configuration files to tell Triton exactly how we wish to use these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d93431",
   "metadata": {},
   "source": [
    "### Starting the server\n",
    "With valid models and configuration files in place, we can now start the server. Below, we do so, use the Python client to wait for it to come fully online, and then check the logs to make sure we didn't get any unexpected warnings or errors while loading the models. Find the host for the PyTorch container in [Setup PyTorch and Triton Containers]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f068c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tritonclient[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1534b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tritonclient.grpc as triton_grpc\n",
    "from tritonclient import utils as triton_utils\n",
    "from requests import get\n",
    "HOST = get('https://api.ipify.org').text\n",
    "PORT = 8001\n",
    "TIMEOUT = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c1a7631",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = triton_grpc.InferenceServerClient(url=f'{HOST}:{PORT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b27273bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for server to come online\n",
    "server_start = time.time()\n",
    "while True:\n",
    "    try:\n",
    "        if client.is_server_ready() or time.time() - server_start > TIMEOUT:\n",
    "            break\n",
    "    except triton_utils.InferenceServerException:\n",
    "        pass\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e27ad9",
   "metadata": {},
   "source": [
    "## Submitting inference requests\n",
    "With our models now deployed on a running Triton server, let's confirm that we get the same results from the deployed model as we get locally. Note that we will occasionally see slight divergences due to floating point errors during parallel execution, but otherwise, results should match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b51ff334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def convert_to_numpy(df):\n",
    "    df = df.copy()\n",
    "    cat_cols = df.select_dtypes('category').columns\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].cat.codes\n",
    "    for col in df.columns:\n",
    "        df[col] =  pd.to_numeric(df[col], downcast='float')\n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb9244ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_pickle('data/X_test.pkl')\n",
    "np_data = convert_to_numpy(X_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eba3b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_predict(model_name, arr):\n",
    "    triton_input = triton_grpc.InferInput('input__0', arr.shape, 'FP32')\n",
    "    triton_input.set_data_from_numpy(arr)\n",
    "    triton_output = triton_grpc.InferRequestedOutput('output__0')\n",
    "    response = client.infer(model_name, model_version='1', inputs=[triton_input], outputs=[triton_output])\n",
    "    return response.as_numpy('output__0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "895a546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_result = triton_predict('model', np_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8eb6415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result computed on Triton: \n",
      "[[0.98703283 0.01296717]\n",
      " [0.9928937  0.00710629]\n",
      " [0.9917474  0.00825262]\n",
      " [0.95682836 0.04317162]\n",
      " [0.99205303 0.00794696]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Result computed on Triton: \")\n",
    "print(triton_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c64c0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result computed locally: \n",
      "[[0.98703283 0.01296715]\n",
      " [0.99289376 0.00710626]\n",
      " [0.9917474  0.00825259]\n",
      " [0.95682836 0.04317162]\n",
      " [0.99205303 0.00794695]]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier()\n",
    "model.load_model(\"/workspace/volume1/model_repository/model/1/xgboost.json\")\n",
    "local_result = model.predict_proba(X_test[0:5])\n",
    "print(\"\\nResult computed locally: \")\n",
    "print(local_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
