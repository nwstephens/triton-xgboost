{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e188c5b",
   "metadata": {},
   "source": [
    "# Deploying with Triton Inference Server\n",
    "\n",
    "Now that we have a model to work with, let's deploy it for real-time serving using Triton. In order to do so, we will need to first serialize the models in the directory structure that Triton expects and then add configuration files to tell Triton exactly how we wish to use these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d62473",
   "metadata": {},
   "source": [
    "### Starting the server\n",
    "With valid models and configuration files in place, we can now start the server. Below, we do so, use the Python client to wait for it to come fully online, and then check the logs to make sure we didn't get any unexpected warnings or errors while loading the models. Find the host for the PyTorch container in [Setup PyTorch and Triton Containers]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bed0ad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tritonclient[all] in /opt/conda/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: python-rapidjson>=0.9.1 in /opt/conda/lib/python3.8/site-packages (from tritonclient[all]) (1.6)\n",
      "Requirement already satisfied: numpy>=1.19.1 in /opt/conda/lib/python3.8/site-packages (from tritonclient[all]) (1.22.3)\n",
      "Requirement already satisfied: geventhttpclient>=1.4.4 in /opt/conda/lib/python3.8/site-packages (from tritonclient[all]) (1.5.3)\n",
      "Requirement already satisfied: grpcio==1.41.0 in /opt/conda/lib/python3.8/site-packages (from tritonclient[all]) (1.41.0)\n",
      "Requirement already satisfied: protobuf>=3.5.0 in /opt/conda/lib/python3.8/site-packages (from tritonclient[all]) (3.19.4)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.8/site-packages (from grpcio==1.41.0->tritonclient[all]) (1.16.0)\n",
      "Requirement already satisfied: gevent>=0.13 in /opt/conda/lib/python3.8/site-packages (from geventhttpclient>=1.4.4->tritonclient[all]) (21.12.0)\n",
      "Requirement already satisfied: brotli in /opt/conda/lib/python3.8/site-packages (from geventhttpclient>=1.4.4->tritonclient[all]) (1.0.9)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from geventhttpclient>=1.4.4->tritonclient[all]) (2021.10.8)\n",
      "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (1.1.2)\n",
      "Requirement already satisfied: zope.event in /opt/conda/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (4.5.0)\n",
      "Requirement already satisfied: zope.interface in /opt/conda/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (5.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (59.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tritonclient[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14a9b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tritonclient.grpc as triton_grpc\n",
    "from tritonclient import utils as triton_utils\n",
    "HOST = '172.18.0.3'\n",
    "PORT = 8001\n",
    "TIMEOUT = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c807c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = triton_grpc.InferenceServerClient(url=f'{HOST}:{PORT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6081d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for server to come online\n",
    "server_start = time.time()\n",
    "while True:\n",
    "    try:\n",
    "        if client.is_server_ready() or time.time() - server_start > TIMEOUT:\n",
    "            break\n",
    "    except triton_utils.InferenceServerException:\n",
    "        pass\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143631e",
   "metadata": {},
   "source": [
    "## Submitting inference requests\n",
    "With our models now deployed on a running Triton server, let's confirm that we get the same results from the deployed model as we get locally. Note that we will occasionally see slight divergences due to floating point errors during parallel execution, but otherwise, results should match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abbd0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def convert_to_numpy(df):\n",
    "    df = df.copy()\n",
    "    cat_cols = df.select_dtypes('category').columns\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].cat.codes\n",
    "    for col in df.columns:\n",
    "        df[col] =  pd.to_numeric(df[col], downcast='float')\n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac2f4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = np.load(\"np_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae68992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_predict(model_name, arr):\n",
    "    triton_input = triton_grpc.InferInput('input__0', arr.shape, 'FP32')\n",
    "    triton_input.set_data_from_numpy(arr)\n",
    "    triton_output = triton_grpc.InferRequestedOutput('output__0')\n",
    "    response = client.infer(model_name, model_version='1', inputs=[triton_input], outputs=[triton_output])\n",
    "    return response.as_numpy('output__0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98c2406e",
   "metadata": {},
   "outputs": [
    {
     "ename": "InferenceServerException",
     "evalue": "[StatusCode.UNAVAILABLE] Request for unknown model: 'model' is not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInferenceServerException\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m triton_result \u001b[38;5;241m=\u001b[39m \u001b[43mtriton_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mtriton_predict\u001b[0;34m(model_name, arr)\u001b[0m\n\u001b[1;32m      3\u001b[0m triton_input\u001b[38;5;241m.\u001b[39mset_data_from_numpy(arr)\n\u001b[1;32m      4\u001b[0m triton_output \u001b[38;5;241m=\u001b[39m triton_grpc\u001b[38;5;241m.\u001b[39mInferRequestedOutput(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput__0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtriton_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtriton_output\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mas_numpy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput__0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tritonclient/grpc/__init__.py:1295\u001b[0m, in \u001b[0;36mInferenceServerClient.infer\u001b[0;34m(self, model_name, inputs, model_version, outputs, request_id, sequence_id, sequence_start, sequence_end, priority, timeout, client_timeout, headers, compression_algorithm)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n\u001b[0;32m-> 1295\u001b[0m     \u001b[43mraise_error_grpc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrpc_error\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tritonclient/grpc/__init__.py:62\u001b[0m, in \u001b[0;36mraise_error_grpc\u001b[0;34m(rpc_error)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_error_grpc\u001b[39m(rpc_error):\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m get_error_grpc(rpc_error) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInferenceServerException\u001b[0m: [StatusCode.UNAVAILABLE] Request for unknown model: 'model' is not found"
     ]
    }
   ],
   "source": [
    "triton_result = triton_predict('model', np_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0028ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import cupy as cp\n",
    "small_model = xgb.XGBClassifier()\n",
    "small_model.load_model(\"model_repository/model/1/xgboost.json\")\n",
    "local_result = small_model.predict_proba(X_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f6e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Result computed on Triton: \")\n",
    "print(triton_result)\n",
    "print(\"\\nResult computed locally: \")\n",
    "print(local_result)\n",
    "cp.testing.assert_allclose(triton_result, local_result, rtol=1e-6, atol=1e-6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
