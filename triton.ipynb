{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e188c5b",
   "metadata": {},
   "source": [
    "# Deploying with Triton Inference Server\n",
    "\n",
    "Now that we have a model to work with, let's deploy it for real-time serving using Triton. In order to do so, we will need to first serialize the models in the directory structure that Triton expects and then add configuration files to tell Triton exactly how we wish to use these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9295727",
   "metadata": {},
   "source": [
    "### Model Serialization\n",
    "Triton models can be stored locally on disk or in S3, Google Cloud Storage, or Azure Storage. For this example, we will stick to local storage, but information about using cloud storage solutions can be found [here](https://github.com/triton-inference-server/server/blob/main/docs/model_repository.md). Each model has a dedicated directory within a main model repository directory. Multiple versions of a model can also be served by Triton, as indicated by numbered directories (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b7cf8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6264d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model repository directory. The name of this directory is arbitrary.\n",
    "REPO_PATH = os.path.abspath('model_repository')\n",
    "os.makedirs(REPO_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87d4de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_model(model, model_name):\n",
    "    # The name of the model directory determines the name of the model as reported\n",
    "    # by Triton\n",
    "    model_dir = os.path.join(REPO_PATH, model_name)\n",
    "    # We can store multiple versions of the model in the same directory. In our\n",
    "    # case, we have just one version, so we will add a single directory, named '1'.\n",
    "    version_dir = os.path.join(model_dir, '1')\n",
    "    os.makedirs(version_dir, exist_ok=True)\n",
    "    \n",
    "    # The default filename for XGBoost models saved in json format is 'xgboost.json'.\n",
    "    # It is recommended that you use this filename to avoid having to specify a\n",
    "    # name in the configuration file.\n",
    "    model_file = os.path.join(version_dir, 'xgboost.json')\n",
    "    model.save_model(model_file)\n",
    "    \n",
    "    return model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2048e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "model = xgb.Booster()\n",
    "model.load_model(\"model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb081ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = serialize_model(model, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5720b3",
   "metadata": {},
   "source": [
    "### The Configuration File\n",
    "The configuration file associated with a model tells Triton a little bit about the model itself and how you would like to use it. You can read about all generic Triton configuration options [here](https://github.com/triton-inference-server/server/blob/master/docs/model_configuration.md) and about configuration options specific to the FIL backend [here](https://github.com/triton-inference-server/fil_backend#configuration), but we will focus on just a few of the most common and relevant options in this example. Below are general descriptions of these options:\n",
    "- **max_batch_size**: The maximum batch size that can be passed to this model. In general, the only limit on the size of batches passed to a FIL backend is the memory available with which to process them. For GPU execution, the available memory is determined by the size of Triton's CUDA memory pool, which can be set via a command line argument when starting the server.\n",
    "- **input**: Options in this section tell Triton the number of features to expect for each input sample.\n",
    "- **output**: Options in this section tell Triton how many output values there will be for each sample. If the \"predict_proba\" option (described further on) is set to true, then a probability value will be returned for each class. Otherwise, a single value will be returned indicating the class predicted for the given sample.\n",
    "- **instance_group**: This determines how many instances of this model will be created and whether they will use the GPU or CPU.\n",
    "- **model_type**: A string indicating what format the model is in (\"xgboost_json\" in this example, but \"xgboost\", \"lightgbm\", and \"tl_checkpoint\" are valid formats as well).\n",
    "- **predict_proba**: If set to true, probability values will be returned for each class rather than just a class prediction.\n",
    "- **output_class**: True for classification models, false for regression models.\n",
    "- **threshold**: A score threshold for determining classification. When output_class is set to true, this must be provided, although it will not be used if predict_proba is also set to true.\n",
    "- **storage_type**: In general, using \"AUTO\" for this setting should meet most usecases. If \"AUTO\" storage is selected, FIL will load the model using either a sparse or dense representation based on the approximate size of the model. In some cases, you may want to explicitly set this to \"SPARSE\" in order to reduce the memory footprint of large models.\n",
    "\n",
    "Based on this information, let's set up configuration files for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "472ef0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum size in bytes for input and output arrays. If you are\n",
    "# using Triton 21.11 or higher, all memory allocations will make\n",
    "# use of Triton's memory pool, which has a default size of\n",
    "# 67_108_864 bytes. This can be increased using the\n",
    "# `--cuda-memory-pool-byte-size` option when the server is\n",
    "# started, but this notebook should work fine with default\n",
    "# settings.\n",
    "MAX_MEMORY_BYTES = 60_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74a22dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cupy as cp\n",
    "X_test = pd.read_pickle(\"X_test.pkl\")\n",
    "y_test = pd.read_pickle(\"y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b12f30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_test.shape[1]\n",
    "num_classes = cp.unique(y_test).size\n",
    "bytes_per_sample = (features + num_classes) * 4\n",
    "max_batch_size = MAX_MEMORY_BYTES // bytes_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9881994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_config(model_dir, deployment_type='gpu', storage_type='AUTO'):\n",
    "    if deployment_type.lower() == 'cpu':\n",
    "        instance_kind = 'KIND_CPU'\n",
    "    else:\n",
    "        instance_kind = 'KIND_GPU'\n",
    "\n",
    "    config_text = f\"\"\"backend: \"fil\"\n",
    "max_batch_size: {max_batch_size}\n",
    "input [                                 \n",
    " {{  \n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {features} ]                    \n",
    "  }} \n",
    "]\n",
    "output [\n",
    " {{\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ {num_classes} ]\n",
    "  }}\n",
    "]\n",
    "instance_group [{{ kind: {instance_kind} }}]\n",
    "parameters [\n",
    "  {{\n",
    "    key: \"model_type\"\n",
    "    value: {{ string_value: \"xgboost_json\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"predict_proba\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"output_class\"\n",
    "    value: {{ string_value: \"true\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"threshold\"\n",
    "    value: {{ string_value: \"0.5\" }}\n",
    "  }},\n",
    "  {{\n",
    "    key: \"storage_type\"\n",
    "    value: {{ string_value: \"{storage_type}\" }}\n",
    "  }}\n",
    "]\n",
    "\n",
    "dynamic_batching {{\n",
    "  max_queue_delay_microseconds: 100\n",
    "}}\"\"\"\n",
    "    config_path = os.path.join(model_dir, 'config.pbtxt')\n",
    "    with open(config_path, 'w') as file_:\n",
    "        file_.write(config_text)\n",
    "\n",
    "    return config_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5974f8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/triton-xgboost/model_repository/model/config.pbtxt'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_config(model_dir, deployment_type='gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d62473",
   "metadata": {},
   "source": [
    "### Starting the server\n",
    "With valid models and configuration files in place, we can now start the server. Below, we do so, use the Python client to wait for it to come fully online, and then check the logs to make sure we didn't get any unexpected warnings or errors while loading the models. Find the host for the PyTorch container in [Setup PyTorch and Triton Containers]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bed0ad8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tritonclient[all] in /opt/conda/lib/python3.8/site-packages (2.20.0)\n",
      "Requirement already satisfied: python-rapidjson>=0.9.1 in /opt/conda/lib/python3.8/site-packages (from tritonclient[all]) (1.6)\n",
      "Requirement already satisfied: numpy>=1.19.1 in /opt/conda/lib/python3.8/site-packages (from tritonclient[all]) (1.22.3)\n",
      "Requirement already satisfied: geventhttpclient>=1.4.4 in /opt/conda/lib/python3.8/site-packages (from tritonclient[all]) (1.5.3)\n",
      "Requirement already satisfied: grpcio==1.41.0 in /opt/conda/lib/python3.8/site-packages (from tritonclient[all]) (1.41.0)\n",
      "Requirement already satisfied: protobuf>=3.5.0 in /opt/conda/lib/python3.8/site-packages (from tritonclient[all]) (3.19.4)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/conda/lib/python3.8/site-packages (from grpcio==1.41.0->tritonclient[all]) (1.16.0)\n",
      "Requirement already satisfied: gevent>=0.13 in /opt/conda/lib/python3.8/site-packages (from geventhttpclient>=1.4.4->tritonclient[all]) (21.12.0)\n",
      "Requirement already satisfied: brotli in /opt/conda/lib/python3.8/site-packages (from geventhttpclient>=1.4.4->tritonclient[all]) (1.0.9)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from geventhttpclient>=1.4.4->tritonclient[all]) (2021.10.8)\n",
      "Requirement already satisfied: greenlet<2.0,>=1.1.0 in /opt/conda/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (1.1.2)\n",
      "Requirement already satisfied: zope.event in /opt/conda/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (4.5.0)\n",
      "Requirement already satisfied: zope.interface in /opt/conda/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (5.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (59.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tritonclient[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14a9b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tritonclient.grpc as triton_grpc\n",
    "from tritonclient import utils as triton_utils\n",
    "HOST = '172.18.0.3'\n",
    "PORT = 8001\n",
    "TIMEOUT = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c807c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = triton_grpc.InferenceServerClient(url=f'{HOST}:{PORT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6081d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for server to come online\n",
    "server_start = time.time()\n",
    "while True:\n",
    "    try:\n",
    "        if client.is_server_ready() or time.time() - server_start > TIMEOUT:\n",
    "            break\n",
    "    except triton_utils.InferenceServerException:\n",
    "        pass\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143631e",
   "metadata": {},
   "source": [
    "## Submitting inference requests\n",
    "With our models now deployed on a running Triton server, let's confirm that we get the same results from the deployed model as we get locally. Note that we will occasionally see slight divergences due to floating point errors during parallel execution, but otherwise, results should match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abbd0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def convert_to_numpy(df):\n",
    "    df = df.copy()\n",
    "    cat_cols = df.select_dtypes('category').columns\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].cat.codes\n",
    "    for col in df.columns:\n",
    "        df[col] =  pd.to_numeric(df[col], downcast='float')\n",
    "    return df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac2f4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data = convert_to_numpy(X_test)\n",
    "#np.save(\"np_data.npy\", np_data)\n",
    "#np_data = np.load(\"np_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae68992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_predict(model_name, arr):\n",
    "    triton_input = triton_grpc.InferInput('input__0', arr.shape, 'FP32')\n",
    "    triton_input.set_data_from_numpy(arr)\n",
    "    triton_output = triton_grpc.InferRequestedOutput('output__0')\n",
    "    response = client.infer(model_name, model_version='1', inputs=[triton_input], outputs=[triton_output])\n",
    "    return response.as_numpy('output__0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98c2406e",
   "metadata": {},
   "outputs": [
    {
     "ename": "InferenceServerException",
     "evalue": "[StatusCode.UNAVAILABLE] Request for unknown model: 'model' is not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInferenceServerException\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m triton_result \u001b[38;5;241m=\u001b[39m \u001b[43mtriton_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36mtriton_predict\u001b[0;34m(model_name, arr)\u001b[0m\n\u001b[1;32m      3\u001b[0m triton_input\u001b[38;5;241m.\u001b[39mset_data_from_numpy(arr)\n\u001b[1;32m      4\u001b[0m triton_output \u001b[38;5;241m=\u001b[39m triton_grpc\u001b[38;5;241m.\u001b[39mInferRequestedOutput(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput__0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtriton_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtriton_output\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mas_numpy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput__0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tritonclient/grpc/__init__.py:1295\u001b[0m, in \u001b[0;36mInferenceServerClient.infer\u001b[0;34m(self, model_name, inputs, model_version, outputs, request_id, sequence_id, sequence_start, sequence_end, priority, timeout, client_timeout, headers, compression_algorithm)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n\u001b[0;32m-> 1295\u001b[0m     \u001b[43mraise_error_grpc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrpc_error\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tritonclient/grpc/__init__.py:62\u001b[0m, in \u001b[0;36mraise_error_grpc\u001b[0;34m(rpc_error)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_error_grpc\u001b[39m(rpc_error):\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m get_error_grpc(rpc_error) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInferenceServerException\u001b[0m: [StatusCode.UNAVAILABLE] Request for unknown model: 'model' is not found"
     ]
    }
   ],
   "source": [
    "triton_result = triton_predict('model', np_data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0028ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import cupy as cp\n",
    "small_model = xgb.XGBClassifier()\n",
    "small_model.load_model(\"model_repository/model/1/xgboost.json\")\n",
    "local_result = small_model.predict_proba(X_test[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f6e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Result computed on Triton: \")\n",
    "print(triton_result)\n",
    "print(\"\\nResult computed locally: \")\n",
    "print(local_result)\n",
    "cp.testing.assert_allclose(triton_result, local_result, rtol=1e-6, atol=1e-6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
