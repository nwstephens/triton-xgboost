{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "790e3997",
   "metadata": {},
   "source": [
    "# Fraud Detection with XGBoost\n",
    "\n",
    "*This notebook is adapted from [Real-time Serving for XGBoost, Scikit-Learn RandomForest, LightGBM, and More](https://developer.nvidia.com/blog/real-time-serving-for-xgboost-scikit-learn-randomforest-lightgbm-and-more/).*\n",
    "\n",
    "### Introduction\n",
    "In this example notebook, we will go step-by-step through the process of training and deploying an XGBoost fraud detection model using Triton's new FIL backend. Along the way, we'll show how to analyze the performance of a model deployed in Triton and optimize its performance based on specific SLA targets or other considerations.\n",
    "\n",
    "### Pre-Requisites\n",
    "You can use Jupyter Lab running on the PyTorch container. If you haven't already installed the the PyTorch see [Set up PyTorch and Triton Containers]()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86f0398",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Fetching Training Data\n",
    "For this example, we will make use of data from the [IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection/overview) Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601ab97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_transaction.csv\n"
     ]
    }
   ],
   "source": [
    "#!tar xzvf data/train_transaction.tgz -C data/\n",
    "train_csv = 'data/train_transaction.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f675e",
   "metadata": {},
   "source": [
    "## Training Example Models\n",
    "While the IEEE-CIS Kaggle competition focused on a more sophisticated problem involving analysis of both fraudulent transactions and the users linked to those transactions, we will use a simpler version of that problem (identifying fraudulent transactions only) to build our example model. In the following steps, we make use of cuML's preprocessing tools to clean the data and then train two example models using XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e693eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.preprocessing import SimpleImputer\n",
    "from cuml.preprocessing import LabelEncoder\n",
    "# Due to an upstream bug, cuML's train_test_split function is\n",
    "# currently non-deterministic. We will therefore use sklearn's\n",
    "# train_test_split in this example to obtain more consistent\n",
    "# results.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ade7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV files into cuDF DataFrames\n",
    "data = cudf.read_csv(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a26d2eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs in data\n",
    "nan_columns = data.columns[data.isna().any().to_pandas()]\n",
    "float_nan_subset = data[nan_columns].select_dtypes(include='float64')\n",
    "\n",
    "imputer = SimpleImputer(missing_values=cp.nan, strategy='median')\n",
    "data[float_nan_subset.columns] = imputer.fit_transform(float_nan_subset)\n",
    "\n",
    "obj_nan_subset = data[nan_columns].select_dtypes(include='object')\n",
    "data[obj_nan_subset.columns] = obj_nan_subset.fillna('UNKNOWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f524fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string columns to categorical or perform label encoding\n",
    "cat_columns = data.select_dtypes(include='object')\n",
    "for col in cat_columns.columns:\n",
    "    data[col] = LabelEncoder().fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faa7bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = data.drop('isFraud', axis=1)\n",
    "y = data.isFraud.astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X.to_pandas(), y.to_pandas(), test_size=0.3, stratify=y.to_pandas(), random_state=SEED\n",
    ")\n",
    "# Copy data to avoid slowdowns due to fragmentation\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9ebb5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00ca9386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model training function\n",
    "def train_model(num_trees, max_depth):\n",
    "    model = xgb.XGBClassifier(\n",
    "        tree_method='gpu_hist',\n",
    "        use_label_encoder=False,\n",
    "        predictor='gpu_predictor',\n",
    "        eval_metric='aucpr',\n",
    "        objective='binary:logistic',\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=num_trees\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_test, y_test)]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b2a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a small model with just 500 trees and a maximum depth of 3\n",
    "model = train_model(500, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87217db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1243"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Free up some room on the GPU by explicitly deleting dataframes\n",
    "import gc\n",
    "del data\n",
    "del nan_columns\n",
    "del float_nan_subset\n",
    "del imputer\n",
    "del obj_nan_subset\n",
    "del cat_columns\n",
    "del X\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c3ee1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for the next step.\n",
    "X_test.to_pickle(\"X_test.pkl\")\n",
    "y_test.to_pickle(\"y_test.pkl\")\n",
    "model.save_model(\"model.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
