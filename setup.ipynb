{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f17318f",
   "metadata": {},
   "source": [
    "# Fraud Detection with XGBoost and Triton-FIL\n",
    "\n",
    "## Introduction\n",
    "In this example notebook, we will go step-by-step through the process of training and deploying an XGBoost fraud detection model using Triton's new FIL backend. Along the way, we'll show how to analyze the performance of a model deployed in Triton and optimize its performance based on specific SLA targets or other considerations.\n",
    "\n",
    "## Pre-Requisites\n",
    "This notebook assumes that you have Docker plus a few Python dependencies. To install all of these dependencies in a conda environment, you may make use of the following conda environment file:\n",
    "```yaml\n",
    "---\n",
    "name: triton_example\n",
    "channels:\n",
    "  - conda-forge\n",
    "  - nvidia\n",
    "  - rapidsai\n",
    "dependencies:\n",
    "  - cudatoolkit=11.4\n",
    "  - cudf=21.12\n",
    "  - cuml=21.12\n",
    "  - cupy\n",
    "  - jupyter\n",
    "  - kaggle\n",
    "  - matplotlib\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - pip\n",
    "  - python=3.8\n",
    "  - scikit-learn\n",
    "  - pip:\n",
    "      - tritonclient[all]\n",
    "      - xgboost>=1.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c9c860",
   "metadata": {},
   "source": [
    "# Set up\n",
    "### Goal\n",
    "\n",
    "1. Build a predictive model with XGBoost\n",
    "2. Deploy that model to Triton Inference Server\n",
    "3. Make predictions\n",
    "\n",
    "### Architecture\n",
    "\n",
    "* Host: GPU (P40 or V100) and CUDA running on Ubuntu 20.4 with plenty of disk space\n",
    "* Triton Container from NGC (open ports 8000, 8001, 8002)\n",
    "* PyTorch Container from NGC (open port 8888)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83552f2d",
   "metadata": {},
   "source": [
    "## Set up Host\n",
    "\n",
    "See host document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cafb9d",
   "metadata": {},
   "source": [
    "## Set up Docker network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo docker network create tritonnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a3b03",
   "metadata": {},
   "source": [
    "## Pull docker containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54826e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triton\n",
    "sudo docker pull nvcr.io/nvidia/tritonserver:22.03-py3\n",
    "sudo docker run --gpus=all -d -p 8000:8000 -p 8001:8001 -p 8002:8002 --network=tritonnet -v /home/azureuser/model_repository:/models \\\n",
    "    --name tritonserver nvcr.io/nvidia/tritonserver:22.03-py3 tritonserver --model-repository=/models\n",
    "\n",
    "# PyTorch\n",
    "sudo docker volume create volume1\n",
    "sudo docker pull nvcr.io/nvidia/pytorch:22.03-py3\n",
    "sudo docker run --gpus=all -t -d -p 8888:8888 --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --network=tritonnet \\\n",
    "    --mount source=volume1,destination=/workspace/triton --name pytorch nvcr.io/nvidia/pytorch:22.03-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c242a4c6",
   "metadata": {},
   "source": [
    "## Identify Triton IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ded950",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo docker network inspect tritonnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be8693",
   "metadata": {},
   "source": [
    "## Start Jupyter Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e3270",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo docker exec -it pytorch /bin/bash\n",
    "nohup jupyter-lab --NotebookApp.token='' --no-browser --port=8888 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a164365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
